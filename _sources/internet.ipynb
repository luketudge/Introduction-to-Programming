{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The internet\n",
    "\n",
    "The internet contains a vast wealth of information, and some of it is even useful. In this lesson we will learn how to retrieve information from the internet in a Python program.\n",
    "\n",
    "## Objective\n",
    "\n",
    "Let's set ourselves a fairly simple task. We would like to automatically retrieve and print out the text of the headline article on a news website. In addition, we would like to find any hyperlinks contained in the article text, and show where those links point to.\n",
    "\n",
    "As an example website, let's use the international edition of [The Guardian](https://www.theguardian.com/international), a mainstream online newspaper. Our first step is to [assign](glossary.ipynb#assignment) a [string](glossary.ipynb#string) variable containing the [URL](glossary.ipynb#URL) (more commonly called 'web address') that points to the main page of the site:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.theguardian.com/international'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy this URL into your web browser and navigate to the page. Keep it open so that you can refer to it and compare what you see in the browser to what you see happening in the Spyder console as you try out the example commands below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## requests\n",
    "\n",
    "You might already have guessed what the next step is. That's right, we need to [import](glossary.ipynb#import) an additional [package](glossary.ipynb#package) containing some new [functions](glossary.ipynb#function). For retrieving data from the internet, the standard most popular package is one called `requests`. It isn't part of the Python [standard library](standard_library.ipynb), but it is very widely used and is included in the default Anaconda installation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important function in the `requests` package, and often the only one we will need, is called `get()`. As the name suggests, it gets the content of a web page. The [argument](glossary.ipynb#argument) is the URL of the page, which in our case we have already stored in a variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what did we get?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "requests.models.Response"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get a `Response` object. This variable contains the content of the webpage (assuming that the URL we entered was a valid one), along with some other information.\n",
    "\n",
    "If you haven't yet had the occasion to learn about how the internet and webpages are structured, you might want to pause for a moment and think about what you are expecting to find once we work out how to get the content of the webpage out of the `Response` object. Will we get an image showing what the webpage looks like when viewed? Or just the plain text content of the page? Or something else?\n",
    "\n",
    "## HTTP\n",
    "\n",
    "To understand what we get from `requests.get()` and how it is organized, let's look very briefly at what happens behind the scenes when an application on our computer, for example a web browser, gets some information, for example a web page, from the internet.\n",
    "\n",
    "When we open a web page in our browser, we just see the content of the page, as if our computer is looking down a tube into the internet and viewing parts of it. But of course what actually happens is rather different. When we navigate to a new web page in our browser, our browser sends out a 'request' for that page. The request is sent via various intermediate computers until reaching a computer on which the page is stored. This computer then sends back a response. In this setup, we say that our browser is the [client](glossary.ipynb#client) program, and the other computer on the internet that controls access to the web page is the [server](glossary.ipynb#server).\n",
    "\n",
    "HTTP (HyperText Transfer Protocol) is a standard procedure prescribing how requests and responses over the internet should be formulated and transmitted. We do not need to know about the details of HTTP. When we surf the web in our browser, the browser handles implementing the requirements of HTTP. And when we get data from the web in a Python program, `requests` handles this. This is one of the benefits of using a well-written pre-made [package](glossary.ipynb#package) like `requests`; it hides away unnecessary complexity inside [functions](glossary.ipynb#function) that allow us to control just a few important aspects of a task. (The slogan for the `requests` project is '[HTTP for Humans](https://requests.readthedocs.io/en/master/)'.)\n",
    "\n",
    "The only thing that we need to know about the details of HTTP is that it prescribes certain 'response status codes'. These codes are short three-digit numbers, each of which has a particular meaning concerning our request. For example, the status code '200' means that our request was successful. This code is contained at the start of the response that we receive from the server. The `requests` package places it in an [attribute](glossary.ipynb#attribute) of the `Response` object called `status_code`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "print(response.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The '200' we see here means that the Guardian's server was able to fulfill our request, and has sent us the web page that we wanted.\n",
    "\n",
    "There are various [other HTTP response codes](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes), but you will commonly encounter only a few of them. There is one in particular that you might be familiar with already, having seen it displayed occasionally in your web browser. It occurs if we request a page that the server does not have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "404\n"
     ]
    }
   ],
   "source": [
    "response2 = requests.get('https://www.theguardian.com/top_secret_prince_philip_sex_tape')\n",
    "print(response2.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'404' means 'not found'.\n",
    "\n",
    "In fact, the `requests` package goes the extra mile for us and also stores an [attribute](glossary.ipynb#attribute) in the `Response` that gives the human-readable meaning of the status code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "print(response.reason)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not Found\n"
     ]
    }
   ],
   "source": [
    "print(response2.reason)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A nice simple flourish that we can add to this first part of our program is a printout confirming the URL of our request (in case we typed it wrong), and the status of our request:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 OK https://www.theguardian.com/international\n"
     ]
    }
   ],
   "source": [
    "print(response.status_code, response.reason, url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HTML\n",
    "\n",
    "As long as our request was successful, the `Reponse` object will also contain the web page or other data that we requested. We can get it as a [string](glossary.ipynb#string) from the `text` [attribute](glossary.ipynb#attribute).\n",
    "\n",
    "Since the web page will probably be quite big, we won't print it all out. Let's instead first assign it into a new variable for convenience, and check how long it is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000230"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page = response.text\n",
    "\n",
    "len(page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's just print out the first thousand characters to see the top of the web page:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<!DOCTYPE html>\n",
      "<html id=\"js-context\" class=\"js-off is-not-modern id--signed-out\" lang=\"en\" data-page-path=\"/international\">\n",
      "<head>\n",
      "<!--\n",
      "     __        __                      _     _      _\n",
      "     \\ \\      / /__    __ _ _ __ ___  | |__ (_)_ __(_)_ __   __ _\n",
      "      \\ \\ /\\ / / _ \\  / _` | '__/ _ \\ | '_ \\| | '__| | '_ \\ / _` |\n",
      "       \\ V  V /  __/ | (_| | | |  __/ | | | | | |  | | | | | (_| |\n",
      "        \\_/\\_/ \\___|  \\__,_|_|  \\___| |_| |_|_|_|  |_|_| |_|\\__, |\n",
      "                                                            |___/\n",
      "    Ever thought about joining us?\n",
      "    https://workforus.theguardian.com/careers/digital-development/\n",
      "     --->\n",
      "<title>News, sport and opinion from the Guardian's global edition | The Guardian</title>\n",
      "<meta charset=\"utf-8\">\n",
      "<meta name=\"description\" content=\"Latest international news, sport and comment from the Guardian\"/>\n",
      "<meta http-equiv=\"X-UA-Compatible\" content=\"IE=Edge\"/>\n",
      "<meta name=\"format-detection\" content=\"telephone=no\"/>\n",
      "<meta name=\"HandheldFriendly\" content=\"Tr\n"
     ]
    }
   ],
   "source": [
    "print(page[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go back to your web browser and compare this with what you see there. It is of course completely different. This is because the web browser displays web pages in a prettified format. The ugly plain text that we are looking at here in Python is what our web browser initially receives from the Guardian server. This text is a set of instructions about how the web page is to be displayed. The browser then implements these instructions, putting colors here, images there, highlighting or linking some of the text, and so on, and then shows us the result. The process of implementing a set of instructions for displaying something is often called 'rendering', and the pretty finished version of the webpage is called the 'rendered' version.\n",
    "\n",
    "That things should be this way makes a certain amount of sense. It would be horribly inefficient for web pages to be stored online as complete, pixel-by-pixel images of the page content, all in color and already laid out. Not only would this require sending very large files across the internet every time someone requested a web page, it would also make web pages inflexible, since they would look the same on any computer or device, no matter the dimensions of its screen or the preferences of its human user. Instead, web pages are stored as fairly minimal instructions about how to display the content of a web page, and it is then up to the web browser program to implement those instructions, and even to ignore or modify some of them in order to adapt the display to a particular device or user, for example making the interface less crowded on a mobile phone, or making the text bigger for a visually-impaired user.\n",
    "\n",
    "So the content of a web page is behind the scenes really like a set of instructions for a web browser. Instructions for a computer must be written in a programming language of some sort. So what programming language are web pages written in? Not Python unfortunately for us, but a new one, called HTML (HyperText Markup Language).\n",
    "\n",
    "Let's take a look at the HTML content of the Guardian web page, and see how the HTML language is structured. The Guardian webpage is a complex one, and the HTML instructions for it are very long, so it would be a bit unwieldy to print it all out in the Python console. Fortunately you can also view the underlying HTML of a web page in your web browser. Go to the Guardian page that you have opened in your browser and try one of the following (which one works will depend on which web browser you are using, but some variant should work in any major browser):\n",
    "\n",
    "* Find a blank area of the web page and click on it with the right button of your mouse. From the menu that appears, select the option 'View Page Source' or something similar-sounding.\n",
    "* If that doesn't work, go to the toolbar of your browser. Click on the main menu button (often at the top right). Select the option 'Page Source' or something similar sounding. This option may be contained in a sub-menu called something like 'Web Developer' or 'Developer Tools'.\n",
    "* As a last resort, go to the address bar of your browser, and in front of the website address prepend the instruction `view-source:`.\n",
    "\n",
    "This should open a separate panel or a whole new tab displaying the plain text HTML of the web page. Something like this:\n",
    "\n",
    "![](images/page_source.png)\n",
    "\n",
    "If you can't make this work, then you can use the following commands in your Spyder console to save the HTML text that we just got into a text file, then you can go and open the text file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('page_source.html', mode='w', encoding='utf-8') as f:\n",
    "    f.write(page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But beware that if you just click on this text file in your file explorer it will most probably open in your web browser and show you the rendered webpage! To see the HTML, open it in a text editor instead.\n",
    "\n",
    "### Tags\n",
    "\n",
    "So what is the general [syntax](glossary.ipynb#syntax) of HTML? The 'M' in HTML stands for 'markup', and a [markup language](glossary.ipynb#markup) is a language that consists mainly of normal human-readable text. In a markup language, the text is 'marked up', that is it is decorated with various surrounding instructions that tell a computer various things about how to display the text. For example, there are instructions for specifying the color and size of the text, for turning text into a clickable link, for inserting images, and so on.\n",
    "\n",
    "In HTML, these additional instructions take the form of 'tags'. Tags mark up some part of the text by enclosing it in an opening tag and a closing tag. An opening tag begins and with the character `<` and ends with `>`. Between these characters is the name of the tag, and possibly some further information. The information in the opening tag says something about how to treat the text that follows. A closing tag also begins and ends with `<` and `>`, but between these characters comes a `/`, followed by the name of the tag that is being closed.\n",
    "\n",
    "For example, on line 15 of the HTML for the Guardian front page there is some text enclosed in opening and closing 'title' tags:\n",
    "\n",
    "> `<title>News, sport and opinion from the Guardian's global edition | The Guardian</title>`\n",
    "\n",
    "These tags say merely 'this text is the title of the document'; they do not in themselves say anything about how to treat that title. Typically, web browsers display the title in the header of the browser window and/or in the tab in which the web page is open. But in principle it is entirely up to the web browser program what to do with tagged text. If we were writing our own web browser app, we could instead write it such that text tagged as 'title' is displayed in massive overlaid letters across the middle of the screen, or is fed into a text-to-speech algorithm and then auto-tuned to the melody of a popular light opera and sung out of the speakers.\n",
    "\n",
    "In the example HTML file [html_examples.html](examples/html_examples.html) you can see some other common HTML tags in action. Remember, if you just download this file and open it, it will probably open in your web browser and show you the rendered web page. To see the underlying text HTML, open the file in a text editor instead.\n",
    "\n",
    "Let's look briefly at a few features of HTML syntax.\n",
    "\n",
    "### Comments\n",
    "\n",
    "Like Python, the HTML language allows for [comments](glossary.ipynb#comment), pieces of text that have no effect on the computer and are there only for human readers. Python comments are simple; any line beginning with the hash symbol `#` is a comment. HTML comments are slightly more complex. HTML marks both the start and the end of a comment. A comment begins with the character combination `<!--` and ends with the combination `--->`. Anything between these two groups of characters is ignored by the web browser.\n",
    "\n",
    "On the Guardian page, you can see a comment near the top of the page containing a recruitment advertisement for web developers.\n",
    "\n",
    "### Hyperlinks\n",
    "\n",
    "One of the most important features of HTML is that it can specify links from one document to another (hyperlinks). Somewhat confusingly, it is not the `<link>` tag that turns a piece of text into a clickable hyperlink (this tag has another role that we will not go into here). Instead it is the `<a>` tag. The 'a' here stands for 'anchor', in the sense of a position in a text being an 'anchor point' that other documents may link to, and that may vice versa link to other documents.\n",
    "\n",
    "So in HTML to turn the piece of plain text 'Complaints & Corrections' into a clickable link, it would be enclosed in tags like this:\n",
    "\n",
    "> `<a>Complaints & corrections</a>`\n",
    "\n",
    "The result looks like this:\n",
    "\n",
    "<a>Complaints & corrections</a>\n",
    "\n",
    "### Attributes\n",
    "\n",
    "Some HTML tags need to specify additional information. For example the `<a>` tag isn't much use without specifying where the hyperlink should point to. On its own, enclosing text in `<a></a>` makes a piece of text behave a bit like a hyperlink (it is highlighted and responds to the mouse hovering over it, and so on), but when the text is clicked nothing happens.\n",
    "\n",
    "To specify additional things about how a piece of tagged text is to be treated, the opening tag may contain one or more 'attributes'. These attributes have specific names and control further details relevant to that type of tag. The syntax for an attribute is to write the attribute name, followed by the equals sign `=`, followed by some value for the attribute. This is syntactically just like [assignment](glossary.ipynb#assignment) in Python, but don't confuse the two; the `=` does not assign a variable in HTML.\n",
    "\n",
    "The 'href' attribute (an abbreviation of 'hypertext reference') of an `<a>` tag specifies the [URL](glossary.ipynb#URL) of the page that the link should point to. So the link to the Guardian's complaints page looks something like this in the underlying HTML:\n",
    "\n",
    "> `<a href=\"https://www.theguardian.com/info/complaints-and-corrections\">Complaints & corrections</a>`\n",
    "\n",
    "And now the link on the resulting web page actually points somewhere:\n",
    "\n",
    "<a href=\"https://www.theguardian.com/info/complaints-and-corrections\">Complaints & corrections</a>\n",
    "\n",
    "## Parsing HTML\n",
    "\n",
    "We now have the HTML content of our target web page, and we know a little bit about HTML. Our next task is to extract the part of the page that we are interested in. The HTML of real-world web pages is often very convoluted and full of all sorts of irrelevant technical and filler information. If you scroll through the HTML of the Guardian front page for example, you will see lots and lots of incomprehensible tags and comments before you get to anything that looks like an article headline.\n",
    "\n",
    "We are looking for a link to the main headline article. The first step in finding it is to find out whether there is a particular recognizable tag or combination of tags that will lead us to this link. For this, it is easiest to use the web browser first, before we do anything in our Python program.\n",
    "\n",
    "Take a look at the rendered webpage in the web browser, and look for the text of the main headline. It will be at the top of a section called 'Headlines', which itself will be near the top of the page). Way back when I wrote this tutorial, the main headline was 'King Henry to divorce Kate of Aragon. Pope says NO!' but of course by the time you come to try out the task the headline will have changed.\n",
    "\n",
    "The headline text will also appear in the HTML somewhere. So make a mental note or copy the text of the headline and head now to the HTML page source. Use your browser's 'find' tool to search for this text. Once you have found the headline text in the HTML, you should see that it appears on a fairly long line of HTML, enclosed in multiple tags. The line will look a bit like this (though I have abbreviated it here somewhat in order to focus on its most important characteristics):\n",
    "\n",
    "> `<h3 class=\"fc-item__title\"><a href=\"https://www.theguardian.com/uk-news/1527/Queen_Kate_divorce_pope_no_no_no\" class=\"fc-item__link\" data-link-name=\"article\"><span class=\"js-headline-text\">King Henry to divorce Kate of Aragon. Pope says NO!</span></a></h3>`\n",
    "\n",
    "HTML allows tags to be 'nested' inside one another. This is what we see here. The text of the main headline is enclosed within a `<span>` tag, within an `<a>` tag, within an `<h3>` tag (or when you view the site the structure of this line may be yet more complex; there are variants depending on the type of the article).\n",
    "\n",
    "For our example task, it is the `<a>` tag that we are ultimately interested in, since its 'href' attribute provides the URL for the link to the actual article page. So let's begin by searching for `<a>` tags in the HTML text of the article that we got from `requests.get()`.\n",
    "\n",
    "### Beautiful Soup\n",
    "\n",
    "How do we go about searching for tags in HTML? We could of course use [string](glossary.ipynb#string) [methods](glossary.ipynb#method) such as `find()` or `index()`. If you have some extra time on your hands and you would like a challenge, you could give this a go. But you will quickly discover that it is very tedious, and you will probably make some mistakes. When we come up against a task that is tedious to accomplish with basic Python and yet is probably a task that many people have had to do already, then we should ask ourselves: Is there a Python [package](glossary.ipynb#package) out there that can do this already?\n",
    "\n",
    "Indeed there is. The most widely-used package for searching in and manipulating HTML is one called 'Beautiful Soup'. This package can chop up an HTML document into its constituent tags and then let us pull out specific tags, their attributes, their enclosed text, and so on. The process of chopping up structured text into its constituent parts is known as [parsing](glossary.ipynb#parse) the text. Beautiful Soup is a package for parsing HTML. The name 'Beautiful Soup' is based on imagining messy 'raw' HTML being turned into a nice tasty 'soup'.\n",
    "\n",
    "For brevity, the actual name of the Beautiful Soup package within Python is `bs4`. Let's [import](glossary.ipynb#import) it. (And yes, `bs4` too is part of the default Anaconda installation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main function that we want from `bs4` is called simply `BeautifulSoup()` (note the uppercase letters). The [argument](glossary.ipynb#argument) is a [string](glossary.ipynb#string) containing the contents of an HTML document. This is what we got from `requests` earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = bs4.BeautifulSoup(page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, let's check the [type](glossary.ipynb#type) of the resulting variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.BeautifulSoup"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This new `BeautifulSoup` object contains our HTML, but with many useful [methods](glossary.ipynb#method) added for picking out specific tags. The methods that we will need all begin with `find_`, so let's look just at these:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "find_all\n",
      "find_all_next\n",
      "find_all_previous\n",
      "find_next\n",
      "find_next_sibling\n",
      "find_next_siblings\n",
      "find_parent\n",
      "find_parents\n",
      "find_previous\n",
      "find_previous_sibling\n",
      "find_previous_siblings\n"
     ]
    }
   ],
   "source": [
    "for meth in dir(soup):\n",
    "    if meth.startswith('find_'):\n",
    "        print(meth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding tags\n",
    "\n",
    "For most simple tasks, the one that we want is `find_all()`. We can use this to find all the tags of a particular kind. The first [argument](glossary.ipynb#argument) is the name of the tag we are looking for, just as a plain string, without the `< >` characters. For example, all the `<a>` tags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_tags = soup.find_all('a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So how many `<a>` tags did we get from the HTML of our Guardian page?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "381"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(a_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly we need to refine our search if we want to pick out just the `<a>` tag for the link to the main headline article. The main way of refining a search for tags is to specify what attributes the tag must have. The additional `attrs` [argument](glossary.ipynb#argument) to `find_all()` can be a [dictionary](glossary.ipynb#dictionary) whose keys are the names of the desired attributes, and whose corresponding values specify what the content of each attribute must be.\n",
    "\n",
    "If you take a look at the HTML, you will see that our target `<a>` tag has two additional attributes 'class' and 'data-link-name':\n",
    "\n",
    "> `<a href=\"...\" class=\"fc-item__link\" data-link-name=\"article\">`\n",
    "\n",
    "So we can narrow our search to tags with these attributes like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_attributes = {'class':'fc-item__link', 'data-link-name':'article'}\n",
    "a_tags = soup.find_all('a', attrs=target_attributes)\n",
    "\n",
    "len(a_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That has narrowed it down considerably, but we still get a lot more than just our one target tag.\n",
    "\n",
    "##### Finding tags within tags\n",
    "\n",
    "If we find that an HTML document contains too many tags that look like the one we want, then often the next step is to start looking *outside* our target tag. An HTML document is usually divided up into sections containing different parts of the text content. You can see this already on the rendered Guardian page; there is a 'Headlines' section, and 'Opinion' section, 'Sport', and so on. So a good option for us here is to search first for the relevant section, and then search within it for the tag we want.\n",
    "\n",
    "A section in HTML is nothing more than a tag. There are tow main tags that are used to mark sections:\n",
    "\n",
    "* `<div>` ('divider') is a very general-purpose division into sections of all kinds. Its effects on the rendered webpage are very varied depending on context.\n",
    "* `<section>` is a surprisingly intuitively-named marker for an actual visual section of a page. At least in most cases.\n",
    "\n",
    "Often these tags may enclose a large chunk of the page content. You may for example see an opening `<div>` tag on one line of the HTML, then see its corresponding closing `</div>` tag only many hundreds of lines later.\n",
    "\n",
    "When looking for a section that encloses the part of the page that we are interested in, we may need to experiment a little and make some initial guesses. Go back to the source of the Guardian page and start carefully scrolling upwards from the line on which our target `<a>` tag appears. You should look for either a `<div>` or a `<section>` tag that has an attribute that looks as though it might be unique to the 'Headlines' section. Eventually you will find one that looks very promising:\n",
    "\n",
    "> `<section id=\"headlines\" ...>`\n",
    "\n",
    "This tag is particularly likely to be the one that marks only the 'Headlines' section because by convention the 'id' attribute of an HTML tag is kept unique. The 'id' attribute is one that the developers of a web page, and our web browser, can use to pick out one specific part of a page and treat it differently if necessary. So if the Guardian's web developers know what they are doing, then there should be only one `<section>` tag with 'id' 'headlines'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headlines = soup.find_all('section', attrs={'id':'headlines'})\n",
    "\n",
    "len(headlines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Success! Although some of the Guardian's journalists have yet to master the English language, its web developers at least get the HTML right.\n",
    "\n",
    "If we are sure that there is only one instance of the kind of tag that we are looking for, or if we anyway only want the first one, then we can switch to the `find()` method. This method [returns](glossary.ipynb#return) just one tag instead of a [list](glossary.ipynb#list):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.element.Tag"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headlines = soup.find('section', attrs={'id':'headlines'})\n",
    "\n",
    "type(headlines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a variable that contains only the part of the document that is enclosed within this `<section>` tag. Searching within this part of the document is as simple as just applying the same `find_` methods to this new variable instead of to the whole 'soup'.\n",
    "\n",
    "So let's see how many article links we now find:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_tags = headlines.find_all('a', attrs=target_attributes)\n",
    "\n",
    "len(a_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better. As a check, let's go through this shorter list of links and take a look at them all:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<a class=\"fc-item__link\" data-link-name=\"article\" href=\"https://www.theguardian.com/world/2020/aug/11/belarus-opposition-candidate-lithuania-protests-svetlana-tikhanouskaya\"><span class=\"fc-item__kicker\">Belarus</span> <span class=\"u-faux-block-link__cta fc-item__headline\"> <span class=\"js-headline-text\">Opposition candidate in Lithuania after second night of protests</span></span> </a>\n",
      "<a class=\"fc-item__link\" data-link-name=\"article\" href=\"https://www.theguardian.com/us-news/2020/aug/10/trump-white-house-shooting-secret-service\"><span class=\"fc-item__kicker\">US</span> <span class=\"u-faux-block-link__cta fc-item__headline\"> <span class=\"js-headline-text\">Secret Service escort Trump from press briefing after shooting outside White House</span></span> </a>\n",
      "<a class=\"fc-item__link\" data-link-name=\"article\" href=\"https://www.theguardian.com/world/live/2020/aug/11/coronavirus-live-news-who-chief-says-its-never-too-late-to-turn-outbreak-around-as-cases-near-20m\"><span class=\"fc-item__kicker fc-item__kicker--breaking-news\"><span class=\"live-pulse-icon flashing-image\"></span>Live</span> <span class=\"u-faux-block-link__cta fc-item__headline\"> <span class=\"js-headline-text\">Coronavirus: Global cases top 20m as WHO chief says ‘it’s never too late to turn outbreak around'</span></span> </a>\n",
      "<a class=\"fc-item__link\" data-link-name=\"article\" href=\"https://www.theguardian.com/world/2020/aug/11/hong-kongs-apple-daily-vows-to-fight-on-after-arrest-of-jimmy-lai\"><span class=\"fc-item__kicker\">Jimmy Lai</span> <span class=\"u-faux-block-link__cta fc-item__headline\"> <span class=\"js-headline-text\">Hong Kong rallies around Apple Daily after arrest of its founder </span></span> </a>\n",
      "<a class=\"fc-item__link\" data-link-name=\"article\" href=\"https://www.theguardian.com/science/2020/aug/10/planet-ceres-ocean-world-sea-water-beneath-surface\"><span class=\"fc-item__kicker\">Space</span> <span class=\"u-faux-block-link__cta fc-item__headline\"> <span class=\"js-headline-text\">Planet Ceres is an 'ocean world' with sea water beneath surface, mission finds</span></span> </a>\n",
      "<a class=\"fc-item__link\" data-link-name=\"article\" href=\"https://www.theguardian.com/world/2020/aug/11/ebo-logging-decree-cameroon-forest-species\"><span class=\"fc-item__kicker\">'Our dead are buried there'</span> <span class=\"u-faux-block-link__cta fc-item__headline\"> <span class=\"js-headline-text\"> Ebo logging decree sparks anger in Cameroon</span></span> </a>\n",
      "<a class=\"fc-item__link\" data-link-name=\"article\" href=\"https://www.theguardian.com/environment/2020/aug/10/gene-manipulation-using-algae-could-grow-more-crops-with-less-water\"><span class=\"fc-item__kicker\">Environment</span> <span class=\"u-faux-block-link__cta fc-item__headline\"> <span class=\"js-headline-text\">Gene manipulation using algae could grow more crops with less water</span></span> </a>\n",
      "<a class=\"fc-item__link\" data-link-name=\"article\" href=\"https://www.theguardian.com/world/2020/aug/10/lebanese-pm-to-resign-after-more-than-a-third-of-cabinet-quits\"><span class=\"fc-item__kicker\">'Chronic corruption'</span> <span class=\"u-faux-block-link__cta fc-item__headline\"> <span class=\"js-headline-text\">Lebanese government quits over Beirut blast</span></span> </a>\n",
      "<a class=\"fc-item__link\" data-link-name=\"article\" href=\"https://www.theguardian.com/us-news/2020/aug/10/chicago-looting-violence-police-shooting-protests\"><span class=\"fc-item__kicker\">US</span> <span class=\"u-faux-block-link__cta fc-item__headline\"> <span class=\"js-headline-text\">Chicago erupts with violence and looting after police shooting</span></span> </a>\n",
      "<a class=\"fc-item__link\" data-link-name=\"article\" href=\"https://www.theguardian.com/australia-news/2020/aug/11/downfall-bp-worker-sacked-for-hitler-meme-wins-200000-in-compensation\"><span class=\"fc-item__kicker\">Downfall</span> <span class=\"u-faux-block-link__cta fc-item__headline\"> <span class=\"js-headline-text\">BP worker sacked for Hitler meme wins $200,000 compensation</span></span> </a>\n",
      "<a class=\"fc-item__link\" data-link-name=\"article\" href=\"https://www.theguardian.com/politics/2020/aug/11/love-of-stilton-drives-wedge-between-uk-and-japan-in-post-brexit-trade-talks\"><span class=\"fc-item__kicker\">All in vein?</span> <span class=\"u-faux-block-link__cta fc-item__headline\"> <span class=\"js-headline-text\">Love of Stilton drives wedge between UK and Japan in trade talks</span></span> </a>\n"
     ]
    }
   ],
   "source": [
    "for tag in a_tags:\n",
    "    print(tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a bit of a mess and not easy to read, because we are seeing the full HTML of each tag, including the tags within the tag, and all the tag attributes.\n",
    "\n",
    "In Beautiful Soup, a tag has a `get_text()` [method](glossary.ipynb#method) that gives us just the plain text that is enclosed in the tag. This can be useful for more easily checking the human-readable part of the HTML:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Belarus  Opposition candidate in Lithuania after second night of protests \n",
      "US  Secret Service escort Trump from press briefing after shooting outside White House \n",
      "Live  Coronavirus: Global cases top 20m as WHO chief says ‘it’s never too late to turn outbreak around' \n",
      "Jimmy Lai  Hong Kong rallies around Apple Daily after arrest of its founder  \n",
      "Space  Planet Ceres is an 'ocean world' with sea water beneath surface, mission finds \n",
      "'Our dead are buried there'   Ebo logging decree sparks anger in Cameroon \n",
      "Environment  Gene manipulation using algae could grow more crops with less water \n",
      "'Chronic corruption'  Lebanese government quits over Beirut blast \n",
      "US  Chicago erupts with violence and looting after police shooting \n",
      "Downfall  BP worker sacked for Hitler meme wins $200,000 compensation \n",
      "All in vein?  Love of Stilton drives wedge between UK and Japan in trade talks \n"
     ]
    }
   ],
   "source": [
    "for tag in a_tags:\n",
    "    print(tag.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can try this in your Spyder console and see the current headlines. You should see that they match up to those displayed in the 'Headlines' section of the rendered webpage.\n",
    "\n",
    "We only want the top headline, and it is reasonable to assume that this will always be the first one in the HTML for the page, so let's switch to `find()` to isolate just the one we want:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<a class=\"fc-item__link\" data-link-name=\"article\" href=\"https://www.theguardian.com/world/2020/aug/11/belarus-opposition-candidate-lithuania-protests-svetlana-tikhanouskaya\"><span class=\"fc-item__kicker\">Belarus</span> <span class=\"u-faux-block-link__cta fc-item__headline\"> <span class=\"js-headline-text\">Opposition candidate in Lithuania after second night of protests</span></span> </a>\n"
     ]
    }
   ],
   "source": [
    "top_headline = headlines.find('a', attrs=target_attributes)\n",
    "\n",
    "print(top_headline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting attributes\n",
    "\n",
    "Now the next step is to get the URL of the link to the main article page. We learned earlier that this is contained in the 'href' attribute of an `<a>` tag. Beautiful Soup's tags can be [indexed](glossary.ipynb#index) with the names of the tag's attributes, rather like a basic Python [dictionary](glossary.ipynb#dictionary):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.theguardian.com/world/2020/aug/11/belarus-opposition-candidate-lithuania-protests-svetlana-tikhanouskaya'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "link = top_headline['href']\n",
    "\n",
    "link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now finally we have what we want from the main page: A link to the page for the top headline article.\n",
    "\n",
    "At this point, it is a good idea to take stock and make a note of what we had to search for in order to find what we wanted. This is something that we can then put into a comment in our finished program as a reminder, in case we later need to refine our search or we find that it does not work as expected the next time the Guardian decides to change the organization of its main page. Something like:\n",
    "\n",
    "> The first `<a>` tag with class \"fc-item__link\" and data-link-name \"article\" within the `<section>` tag with id \"headlines\".\n",
    "\n",
    "We haven't yet finished our target program. We said that we wanted to get the text of the main article, and the URLs of all the links contained in the main article. For this, we will need to now take the URL we have just found, which points to the main article page, `get()` this page using `requests`, then search again in the HTML of the page in order to find the article text and links. However, all of this involves only small variations on the techniques we have already covered. So if you would like to practice, you can try to finish off the example task yourself starting from here. The example [script](glossary.ipynb#script) [guardian_top_article.py](examples/guardian_top_article.py) shows one possible solution.\n",
    "\n",
    "## APIs\n",
    "\n",
    "The task that we just carried out is often called [web scraping](glossary.ipynb#scraping). We accessed a website using our own computer program rather than a web browser, and our program extracted some information from the page.\n",
    "\n",
    "You should be aware that not all websites are particularly happy about being scraped. Some websites take actions to block this kind of access. Even for those websites that do not explicitly block activity that looks like scraping, it is important as a matter of good 'netiquette' to check first whether a website already provides its own guidelines and methods for programs that want to access its data. When the owner of a web site or other web service provides a specific channel through which other non-browser programs may access data, this system is termed an [API](glossary.ipynb#API) ('Application Programming Interface'). An API is a system with a fixed set of rules that state what information a website will send back to another computer program when that computer program submits requests in a specific format.\n",
    "\n",
    "The Guardian website, for example, in fact provides a more structured way for us to access the content of its articles, via an API. You can read a bit more about it at the [Guardian API documentation page](https://open-platform.theguardian.com/documentation). The Guardian's API asks our program to go to the following URL if we want to search for articles:\n",
    "\n",
    "`'https://content.guardianapis.com/search'`\n",
    "\n",
    "We should then add one or more 'parameters' to the URL in order to refine our search. The [syntax](glossary.ipynb#syntax) for adding parameters to a URL is to place a `?` character after the main URL, then name the parameters and their values using `=`, separating multiple parameters with `&`. The Guardian API allows for the parameters 'q', to specify a search word, and 'from-date', to specify a date at which to begin the search. So the URL for a request for articles about taramasalata published since 2014 would look like this:\n",
    "\n",
    "`https://content.guardianapis.com/search?q=taramasalata&from-date=2014-01-01`\n",
    "\n",
    "However, rather than try to put together the whole URL ourselves and probably misplace some vital punctuation somewhere, we can let the `requests` package do it for us. The `get()` function has an optional `params` [argument](glossary.ipynb#argument) that can be a [dictionary](glossary.ipynb#dictionary) of parameters for the URL. Like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_url = 'https://content.guardianapis.com/search'\n",
    "query = {'q':'taramasalata', 'from-date':'2014-01-01'}\n",
    "\n",
    "response = requests.get(api_url, params=query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API keys\n",
    "\n",
    "However, something is not quite right when we check the status of our request to the Guardian's API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "401 Unauthorized\n"
     ]
    }
   ],
   "source": [
    "print(response.status_code, response.reason)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look back at the [documentation page](https://open-platform.theguardian.com/documentation) for the API. It states that we first need something called an API *key*, and then send the key along with our request.\n",
    "\n",
    "This is a fairly common requirement of APIs. Most, though not all, require everyone that uses the API to first register some information, rather like registering for any other website that provides a service. This requirement allows the owners of the website to keep track of what their content is being used for, and to limit how much use each person can make of it for free.\n",
    "\n",
    "If you look at the Guardian's API [documentation page on keys](https://open-platform.theguardian.com/access) you can see the terms and conditions of free or commercial use. For example, the free key limits how many requests for data you can make per second and per day. If you would like, you can sign up for a free developer key via this page. You will get a key sent to you. It is just a string of characters. Save this to a text file somewhere safe on your computer, and then you can use it to test out the Guardian API if you want to follow the final examples below.\n",
    "\n",
    "To get an API key into Python, we can load it from the text file we saved it in. We learned how to load text from files in the [lesson on files](files.ipynb). I have temporarily put mine in the file '.guardian_api_key.txt' in my current Python working directory. (Note that because a text file may often contain an invisible extra [newline character](glossary.ipynb#newline), it is safest to strip away any surrounding [whitespace](glossary.ipynb#whitespace) just in case.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = open('.guardian_api_key.txt').read().strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the API states, this key should be sent as the 'api-key' parameter in our program's request. So like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 OK\n"
     ]
    }
   ],
   "source": [
    "query['api-key'] = api_key\n",
    "\n",
    "response = requests.get(api_url, params=query)\n",
    "\n",
    "print(response.status_code, response.reason)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it works!\n",
    "\n",
    "So what answer did we get? Let's take a look at the text of the response, as we did when we just accessed the front page earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"response\":{\"status\":\"ok\",\"userTier\":\"developer\",\"total\":47,\"startIndex\":1,\"pageSize\":10,\"currentPage\":1,\"pages\":5,\"orderBy\":\"relevance\",\"results\":[{\"id\":\"food/2020/jul/15/17-dips-arent-hummus-humungous-salsa-guacamole-taramasalata-baba-ganoush\",\"type\":\"article\",\"sectionId\":\"food\",\"sectionName\":\"Food\",\"webPublicationDate\":\"2020-07-15T10:30:24Z\",\"webTitle\":\"Lucky dip: 17 delicious dunkable snacks that aren’t hummus\",\"webUrl\":\"https://www.theguardian.com/food/2020/jul/15/17-dips-arent-hummus-humungous-salsa-guacamole-taramasalata-baba-ganoush\",\"apiUrl\":\"https://content.guardianapis.com/food/2020/jul/15/17-dips-arent-hummus-humungous-salsa-guacamole-taramasalata-baba-ganoush\",\"isHosted\":false,\"pillarId\":\"pillar/lifestyle\",\"pillarName\":\"Lifestyle\"},{\"id\":\"food/2019/nov/30/scallop-roe-alternative-use-taramasalata-recipe-waste-not-tom-hunt\",\"type\":\"article\",\"sectionId\":\"food\",\"sectionName\":\"Food\",\"webPublicationDate\":\"2019-11-30T06:00:28Z\",\"webTitle\":\"An alternative use for scallop roe\",\"webUrl\":\"https://www.theguardian.com/food/2019/nov/30/scallop-roe-alternative-use-taramasalata-recipe-waste-not-tom-hunt\",\"apiUrl\":\"https://content.guardianapis.com/food/2019/nov/30/scallop-roe-alternative-use-taramasalata-recipe-waste-not-tom-hunt\",\"isHosted\":false,\"pillarId\":\"pillar/lifestyle\",\"pillarName\":\"Lifestyle\"},{\"id\":\"lifeandstyle/2019/nov/26/grand-aioli-the-dinner-to-cook-when-you-cant-be-bothered-cooking\",\"type\":\"article\",\"sectionId\":\"lifeandstyle\",\"sectionName\":\"Life and style\",\"webPublicationDate\":\"2019-11-25T17:00:01Z\",\"webTitle\":\"Grand aioli: the dinner to cook when you can't be bothered cooking\",\"webUrl\":\"https://www.theguardian.com/lifeandstyle/2019/nov/26/grand-aioli-the-dinner-to-cook-when-you-cant-be-bothered-cooking\",\"apiUrl\":\"https://content.guardianapis.com/lifeandstyle/2019/nov/26/grand-aioli-the-dinner-to-cook-when-you-cant-be-bothered-cooking\",\"isHosted\":false,\"pillarId\":\"pillar/lifestyle\",\"pillarName\":\"Lifestyle\"},{\"id\":\"food/2019/dec/14/how-turn-christmas-party-leftovers-mexican-brunch\",\"type\":\"article\",\"sectionId\":\"food\",\"sectionName\":\"Food\",\"webPublicationDate\":\"2019-12-14T06:00:15Z\",\"webTitle\":\"How to turn Christmas party leftovers into a Mexican brunch | Waste not\",\"webUrl\":\"https://www.theguardian.com/food/2019/dec/14/how-turn-christmas-party-leftovers-mexican-brunch\",\"apiUrl\":\"https://content.guardianapis.com/food/2019/dec/14/how-turn-christmas-party-leftovers-mexican-brunch\",\"isHosted\":false,\"pillarId\":\"pillar/lifestyle\",\"pillarName\":\"Lifestyle\"},{\"id\":\"food/2020/feb/28/the-melusine-london-e1-restaurant-review-grace-dent\",\"type\":\"article\",\"sectionId\":\"food\",\"sectionName\":\"Food\",\"webPublicationDate\":\"2020-02-28T09:30:13Z\",\"webTitle\":\"The Melusine, London E1: ‘A small kitchen turning out magic’ – restaurant review | Grace Dent\",\"webUrl\":\"https://www.theguardian.com/food/2020/feb/28/the-melusine-london-e1-restaurant-review-grace-dent\",\"apiUrl\":\"https://content.guardianapis.com/food/2020/feb/28/the-melusine-london-e1-restaurant-review-grace-dent\",\"isHosted\":false,\"pillarId\":\"pillar/lifestyle\",\"pillarName\":\"Lifestyle\"},{\"id\":\"food/2019/oct/17/flaccid-croissants-oil-drenched-carbs-train-jay-rayner\",\"type\":\"article\",\"sectionId\":\"food\",\"sectionName\":\"Food\",\"webPublicationDate\":\"2019-10-17T11:00:26Z\",\"webTitle\":\"Flaccid croissants, oil-drenched carbs. Yes, I’m on the road again\",\"webUrl\":\"https://www.theguardian.com/food/2019/oct/17/flaccid-croissants-oil-drenched-carbs-train-jay-rayner\",\"apiUrl\":\"https://content.guardianapis.com/food/2019/oct/17/flaccid-croissants-oil-drenched-carbs-train-jay-rayner\",\"isHosted\":false,\"pillarId\":\"pillar/lifestyle\",\"pillarName\":\"Lifestyle\"},{\"id\":\"food/2020/jan/09/fitbit-for-chickpeas-zipcar-for-leftovers-the-food-tech-we-really-need-in-the-2020s\",\"type\":\"article\",\"sectionId\":\"food\",\"sectionName\":\"Food\",\"webPublicationDate\":\"2020-01-09T08:00:02Z\",\"webTitle\":\"Fitbit for chickpeas, Zipcar for leftovers: the food tech we really need in the 2020s\",\"webUrl\":\"https://www.theguardian.com/food/2020/jan/09/fitbit-for-chickpeas-zipcar-for-leftovers-the-food-tech-we-really-need-in-the-2020s\",\"apiUrl\":\"https://content.guardianapis.com/food/2020/jan/09/fitbit-for-chickpeas-zipcar-for-leftovers-the-food-tech-we-really-need-in-the-2020s\",\"isHosted\":false,\"pillarId\":\"pillar/lifestyle\",\"pillarName\":\"Lifestyle\"},{\"id\":\"science/2019/dec/02/did-you-solve-it-smart-as-a-box-of-frogs\",\"type\":\"article\",\"sectionId\":\"science\",\"sectionName\":\"Science\",\"webPublicationDate\":\"2019-12-02T17:10:09Z\",\"webTitle\":\"Did you solve it? Smart as a box of frogs\",\"webUrl\":\"https://www.theguardian.com/science/2019/dec/02/did-you-solve-it-smart-as-a-box-of-frogs\",\"apiUrl\":\"https://content.guardianapis.com/science/2019/dec/02/did-you-solve-it-smart-as-a-box-of-frogs\",\"isHosted\":false,\"pillarId\":\"pillar/news\",\"pillarName\":\"News\"},{\"id\":\"food/2019/sep/13/10-heddon-st-london-w1-restaurant-review-grace-dent\",\"type\":\"article\",\"sectionId\":\"food\",\"sectionName\":\"Food\",\"webPublicationDate\":\"2019-09-13T09:00:17Z\",\"webTitle\":\"10 Heddon St, London W1: ‘We'll never tire of exemplary pasta’ – restaurant review\",\"webUrl\":\"https://www.theguardian.com/food/2019/sep/13/10-heddon-st-london-w1-restaurant-review-grace-dent\",\"apiUrl\":\"https://content.guardianapis.com/food/2019/sep/13/10-heddon-st-london-w1-restaurant-review-grace-dent\",\"isHosted\":false,\"pillarId\":\"pillar/lifestyle\",\"pillarName\":\"Lifestyle\"},{\"id\":\"books/2019/nov/30/best-cookbooks-and-food-writing-of-2019\",\"type\":\"article\",\"sectionId\":\"books\",\"sectionName\":\"Books\",\"webPublicationDate\":\"2019-11-30T08:00:31Z\",\"webTitle\":\"Best cookbooks and food writing of 2019\",\"webUrl\":\"https://www.theguardian.com/books/2019/nov/30/best-cookbooks-and-food-writing-of-2019\",\"apiUrl\":\"https://content.guardianapis.com/books/2019/nov/30/best-cookbooks-and-food-writing-of-2019\",\"isHosted\":false,\"pillarId\":\"pillar/arts\",\"pillarName\":\"Arts\"}]}}\n"
     ]
    }
   ],
   "source": [
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yet again this looks like a bit of a mess. But if you stop your eyes from glazing over for a moment and look closely, you will notice that this is not an HTML mess. It is something else. Do you recognize it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPOILER ALERT SPOILER ALERT SPOILER ALERT SPOILER ALERT SPOILER ALERT SPOILER ALERT SPOILER ALERT SPOILER ALERT SPOILER ALERT SPOILER ALERT SPOILER ALERT SPOILER ALERT SPOILER ALERT SPOILER ALERT SPOILER ALERT SPOILER ALERT SPOILER ALERT SPOILER ALERT SPOILER ALERT SPOILER ALERT SPOILER ALERT SPOILER ALERT SPOILER ALERT SPOILER ALERT SPOILER ALERT SPOILER ALERT SPOILER ALERT SPOILER ALERT SPOILER ALERT SPOILER ALERT SPOILER ALERT SPOILER ALERT SPOILER ALERT SPOILER ALERT SPOILER ALERT SPOILER ALERT SPOILER ALERT SPOILER ALERT SPOILER ALERT SPOILER ALERT SPOILER ALERT SPOILER ALERT SPOILER ALERT SPOILER ALERT SPOILER ALERT SPOILER ALERT SPOILER ALERT SPOILER ALERT SPOILER ALERT SPOILER ALERT SPOILER ALERT SPOILER ALERT SPOILER ALERT SPOILER ALERT SPOILER ALERT SPOILER ALERT SPOILER ALERT SPOILER ALERT SPOILER ALERT SPOILER ALERT SPOILER ALERT SPOILER ALERT SPOILER ALERT SPOILER ALERT SPOILER ALERT SPOILER ALERT SPOILER ALERT SPOILER ALERT SPOILER ALERT SPOILER ALERT SPOILER ALERT SPOILER ALERT SPOILER ALERT SPOILER ALERT SPOILER ALERT SPOILER ALERT SPOILER ALERT SPOILER ALERT SPOILER ALERT SPOILER ALERT SPOILER ALERT SPOILER ALERT SPOILER ALERT SPOILER ALERT SPOILER ALERT SPOILER ALERT SPOILER ALERT SPOILER ALERT SPOILER ALERT SPOILER ALERT SPOILER ALERT SPOILER ALERT SPOILER ALERT SPOILER ALERT SPOILER ALERT SPOILER ALERT SPOILER ALERT SPOILER ALERT SPOILER ALERT SPOILER ALERT \n"
     ]
    }
   ],
   "source": [
    "print('SPOILER ALERT ' * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's right. It's a JSON mess! Remember [JSON](glossary.ipynb#JSON)? JSON is a text data format that we met in the [lesson on files](files.ipynb#JSON). It shares the curly bracket and square bracket [syntax](glossary.ipynb#syntax) of Python [dictionaries](glossary.ipynb#dictionary) and [lists](glossary.ipynb#list).\n",
    "\n",
    "The Guardian's API by default sends back some JSON rather than HTML. This is fairly common; since APIs are intended for use by computer programs, many of them send data in JSON format because this is a format that is easy for computer programs to make use of. All the extra fancy formatting instructions of HTML are not necessary if the text is only to be consumed by a computer program.\n",
    "\n",
    "The `requests` `Response` object has a [method](glossary.ipynb#method) for converting any JSON data that is received into Python lists and dictionaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_data = response.json()\n",
    "\n",
    "type(json_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can explore what we got, using basic Python list and dictionary techniques:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['response'])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['status', 'userTier', 'total', 'startIndex', 'pageSize', 'currentPage', 'pages', 'orderBy', 'results'])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_data = json_data['response']\n",
    "\n",
    "json_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = json_data['results']\n",
    "\n",
    "type(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'food/2020/jul/15/17-dips-arent-hummus-humungous-salsa-guacamole-taramasalata-baba-ganoush',\n",
       " 'type': 'article',\n",
       " 'sectionId': 'food',\n",
       " 'sectionName': 'Food',\n",
       " 'webPublicationDate': '2020-07-15T10:30:24Z',\n",
       " 'webTitle': 'Lucky dip: 17 delicious dunkable snacks that aren’t hummus',\n",
       " 'webUrl': 'https://www.theguardian.com/food/2020/jul/15/17-dips-arent-hummus-humungous-salsa-guacamole-taramasalata-baba-ganoush',\n",
       " 'apiUrl': 'https://content.guardianapis.com/food/2020/jul/15/17-dips-arent-hummus-humungous-salsa-guacamole-taramasalata-baba-ganoush',\n",
       " 'isHosted': False,\n",
       " 'pillarId': 'pillar/lifestyle',\n",
       " 'pillarName': 'Lifestyle'}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and so on.\n",
    "\n",
    "That's it for our short introduction to the internet. Try an exercise.\n",
    "\n",
    "## Exercise\n",
    "\n",
    "The website [Etymonline.com](https://www.etymonline.com) provides information on the origin of words in English. It is quite fun to browse.\n",
    "\n",
    "Near the top of the webpage there is a short list of 'trending' words, words that many users are currently searching for. Write a script that finds and prints out the current trending words. At the time I wrote this, there was no API for Etymonline, so you can use the basic scraping techniques from the first part of this lesson.\n",
    "\n",
    "As we learned, you will need to check the HTML source of the page in your browser first and identify the tags or combination of tags that you are looking for, and perhaps some attributes that will help you to single them out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "try:\n",
    "    os.remove('page_source.html')\n",
    "except FileNotFoundError:\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
